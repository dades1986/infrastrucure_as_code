
************terraform state*********

$ ls terraform-local-file
main.tf  variables.tf

******main.tf********
resource "local_file" "pet" {
filename = var.filename
content = var.content
}

******variables.tf********

variable "filename"{
 default = "/root/pets.txt"
}

variable "content" {
 default = "I love pets!"
}

$ cd   terraform-local-file
$ terraform init : create plugins
$terraform plan: terraform try to refresh the state in-memory prior to the plan, since the first time 
we are runing the terraform plan command, there will be no details related to a state printed
therfore impliyin that there is no state recorded at this moment in time, from this , terraform also understand  that 
currently there are no resources provisioned based on the configuration files, and then creates an "execution plan" of create

$ terraform apply : this command try also refresh the in-memory state, finds that there is no state recorded at the moment
and proceeds to create an aexecution plan, once confirmed "yes", terraform creates the local file resource as expected,
terraform creates a unique id

$ cat /root/oets.txt
I love pets!

$ terraform apply again :Terraform knows that a local file and the same id that we just saw already exists
and it takes no further action, so how does terraform know that?
how does it know that the local file resource already exists?
to understand that, let us check the cotents of the configuration directory again

$ ls
main.tf variables.tf terraform.tfstate
terraform.tfstate : the consequence of terraform apply command (first use) that created the resource in the first place : created before at least one .

resource "tls_private_key" "pvtkey" {
 algorithm = "RSA"
 rsa_bits  = "4096"
}
-----------------------------------------------------
resource "tls_private_key" "pvtkey" {
 algorithm = "RSA"
 rsa_bits  = "4096"
}

resource "local_file" "key_details"{
 filename = "/root/key.txt"
 content  = "${tls_private_key.pvtkey.private_key_pem}"
}
-------
the state file as a json data structure, that maps the

real-world infrastructure resources------------<state file json>----------resource definition in the configuration file (targeted state)

***************purpose of state ***********
*terraform use stae file, to map the resource configuration (main.tf) to the real world infrastructure
*this mapping allows terraform to create execution plans when the drift is identified between the resource configuration file (main.tf) and the state (terraform.tfstate).
* hence, a stae file can be considered to be a "blue print" of all the resources that are for managers out there in the real world
*when terraform creates a resource, it records its identity in the state,be  it  (que ce soit)"local file resource" that creates a file in the "machine"
a logical resource such as th "random pet resource" which just throws out the random pet name or resources in the cloud, each resource created and managed by terraform 
would have a unique ID which is used to identify the resources in the real world.

besides the mappiing between resources and the the configuration and the real world, the state file also tracks metadata details such as "resource dependencies".

* deleting dependant resources : (tracking metadata)if we delete dependant resources in the configuration file (main.tf), terraform use a stae file to delete resources depending in information in the state
*perfermances :when dealing with the handful (petite quantité) number of resources,it may be feasible for terraform to reconcile state

Terraform refresh: Reconciling real-world drift (resource crash, change inside a vm, changes due to an other conf tool like ansible...)
Prior to a plan or apply operation, Terraform does a refresh to "update the state file with real-world status". You can also do a refresh any time with
terraform refresh

$ terraform refresh
aws_instance.example: Refreshing state... (ID: i-011a9893eff09ede1)

What Terraform is doing here is "reconciling the resources tracked by the state file" with "the real world". 

It does this by querying your infrastructure "providers" to find out what's actually running and the current configuration,

and "updating the state file" with this "new information". 

Terraform is designed to co-exist with "other tools" as well as" manually provisioned resources" and so it only "refreshes resources under its management".

The output for a refresh is minimal. Terraform lists each resource it is refreshing along with its internal ID. 

Running refresh does not modify infrastructure, but does modify the state file.

If the state has "drifted from the last time Terraform ran", refresh allows that drift to be "detected".

By default, a backup of your state file is written to "terraform.tfstate.backup" in case the state file is" lost" or "corrupted" to simplify "recovery".

Terraform stores a cache of attribute values for all resources in the state

$ terraform plan --refresh=false

*collaboration in the team :save state file in remote datat store, state to be shared by memebers, : hashicorp consul, terraform cloud oe aws s3

*plan and apply refresh the state

*********************terraform state considerations******************

* Terraform state in the single source of truth for terraform to understand what is deployed in the real world
* State is non-optional feature in Terraform
*state file contanis sensitive information
*it contains every little detail about our infrastructure: cpu,memory,size of disk, ip, ssh keypad--->database : initial password 
*remote state in github, gitlab, bitbucket
*sensitive state : aws3 , google cloud storage,azure storeage, terraform cloud


**********************terraform commands ***************
* terraform validate : validate syntax configuration
*terraform fmt : format canonical 
*terraform show -json
*terraform providers
*terraform providers mirro /root/terraform/new_local_file
*terraform output
*terraform refresh : refresh the state file , not a real world
*terraform plan
*terraform graph (graphviz -y)
*apt install graphviz
*terraform graph | dot -Tsvg > graph.svg  ==>chrome

apt update
$ apt install graphviz -y
$ terraform graph | dot -Tsvg > graph.svg



********************mutable Vs Immutable Infrastructure****************


* in-place update :

*when terraform updates a resources , such as updating the permissions of a local file, it first festroys it and then recreate it with new permission
why terraform do that ?

consider an application server running NGINX with the version of 1.17
when a new version of NGINX is released, we upgrade the software running on this web server
first from v1.17 to v1.18, and eventually, when a new version v1.19 is released, we upgraded the same way from v1.18 to v1.19
this can be done using a number of different ways, one simple approch is to download the desired version of NGINX
and then use it to manually upgrade the software on the web server, during a maintenance window.
of course , we can also make use of tools such as ad-hoc scripts,or configuration management tools such as Ansible to achiene this.

for  high availibility, instead of relying on one web server, we can have a pool of theses web servers all runing the same software and code.(1.17, 1.17, 1.17)
we would have to use the same software upgrade lifecycle for each of the servers, using the same approach that we use for the first web server.
this type of update is known as "in-place update".and this because the underlying infrastructure remains the same
but the software and the configuration on these servers are changed as part of the update. and here is an examlpe of a mutable infrastructure.

*********************** "configuration drift"*************

updating software on a system can be a complex task,and in almost all cases, there are bound to be a set of dependencies
that have to be met before an upgrade can be carried out successfully.

let us assume that web server 1 and web server 2 have every dependency met, while we try to upgrade the version from 1.18 to 1.19
as a result, these two servers are upgraded without any issues. web server 3 in the other hand , does not .
the upgrade fails on web server 3 becasue a few dependancies that are not met, and as a result, it remains at version 1.18 (1.19, 1.19,1.18)
a failure in upgrade could be because of a number of reasons :
-network issues impacting the connectivity to the software repository
-file system full
-or different version  of operating system running on the web server 3, as compared to the other two.

over time, with multiple updates and changes to this pool of three servers, there is a possibility that each of the servers vary from another
may be in software, or configuration or operating system etc, this is known as a "configuration drift"

****************immutable infastructure**********************

this configuration drift, can leave the infrastructure in a complex state. making it difficult to plan and carry out subsequent updates
troubleshooting issues would also be a difficult task as each server would behave slightly differently from the other because of this configuration drift.

Instead of updating the software versions on the web servers, we can spin up a new web servers with the updated software version
and then delete the old web server ( avoir 4 dans cet exemple , ensuite enlever l'ancien : remplacement)

so when we want to update NGINX 1.17 to 1.18, a new server is provisioned with 1.18 version of NGINX, if the upgrade goes through, then the old web server is deleted.
that's called immutable infrastructure, immutable means unchanged or something that you can not change.
as a consequence, with the immutable infrastructure, we can not carry out "in-place" updates of the resources anymore
this doesn't mean that updating web servers this way will not lead to failures, if the upgrade fails for any reason,
the old web server will left intact and the failed server will be removed, as result we do not leave much room for configuration drit to occur between our servers
ensuring that it is left in a simple easy-to-understand state.

Since we are working with the infrastructure as code, immutability makes it easier to version the infrastructure and to roll back and roll farward between versions.
Terraform as an infrastructure provisionning tool uses this approach

resource "local_file" "pet" {
filename = "/root/pets.txt"
content = "we love pets"
file_permission (added one) = "0700"   777-->700
}

$ terraform apply

original file with the permission 777  "/root/pets.txt" to be deleted and a new file  "/root/pets.txt" to be created with the new permission 700.
By default, terraform destroys the resource first , before creatin a new one in its place.

but if we want the resource to be created first before the old one is deleted or to ignore deletion completely, how do we do that?
these can be done by making use of lifecycle rules in our resource block. and we will see how to do that next.



resumé: mutable infrastructure <--------> configuration drift
        immutable infrastructure <-------> no configuration drift














